<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>buildupchao | 各自努力 | 顶峰相见</title>
    <description>本站是buildupchao的技术分享博客。</description>
    <link>http://www.buildupchao.cn/</link>
    <atom:link href="http://www.buildupchao.cn/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 10 Dec 2020 19:08:01 +0800</pubDate>
    <lastBuildDate>Thu, 10 Dec 2020 19:08:01 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>在大数据时代，我们缺乏的到底是思维还是能力？</title>
        <description>&lt;p&gt;大学最重要的事情应该是锻炼自学的能力、培养自律的心性。
&lt;br /&gt;&lt;br /&gt;
工作后，最重要的事情应该是执行力MAX的可靠性以及严谨处事的态度。
&lt;br /&gt;&lt;br /&gt;
似乎每件事都会有专门的目标性。
&lt;br /&gt;&lt;br /&gt;
然而，工作久了，难免会&lt;strong style=&quot;color:red;&quot;&gt;“学会偷懒”&lt;/strong&gt;，不再像从前哐哧哐哧就开始无想法的行动。&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;1按部就班固化思维&quot;&gt;1.按部就班固化思维&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong style=&quot;color:blue;&quot;&gt;在遇到采集数据异常排查问题时，W总是习惯于从文件系统拉取log日志进行查询，然而采集机器数少则十几台，多则成百上千&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;不是吧不是吧，这种耗时费力的事情，不会真的有人这么干吧？&lt;/li&gt;
      &lt;li&gt;想必除了想要摸鱼的人，不会有人傻到大批量拉取日志进行分析。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;正解：可以借助于Hive or Flink集群建表SQL分析，效率更高。（ 当然，有更方便快捷的方式欢迎推荐。）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2从零造轮子为哪般&quot;&gt;2.从零造轮子为哪般&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong style=&quot;color:blue;&quot;&gt;有工具可以辅助快速解决问题，然而经常习惯于自己从零造轮子？&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;没错，说的就是你，有极速模式，为何要采用常规流程模式呢？难道是不想有自己的空闲时间充电学习？&lt;/li&gt;
      &lt;li&gt;难道说你是在故意放松，给你拖延工作量，从而有时间跳槽走位？？？
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;正解：我们可以学习架构原理，运用思维，发散学习。（比如二叉树的中序遍历可以运用到自助式ETL中的表达式聚合计算中）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;3架构缺陷补丁不断&quot;&gt;3.架构缺陷补丁不断&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong style=&quot;color:blue;&quot;&gt;小文件合并功能，采用单纯的JAVA服务自己实现，每天平均有80W+的小文件合并任务量？？？&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;如果数据都是存储在HDFS上的，那么你的NameNode压力是要多大啊？&lt;/li&gt;
      &lt;li&gt;大量小文件问题根本原因还是架构设计有问题，如果日志切分合理，小文件量是极其少的。&lt;/li&gt;
      &lt;li&gt;所以，你的工作量就是自己造就了架构问题，然后在此基础之上再开发一套服务来做小日志文件归并？别人从复杂入简，而你是从简单到复杂。（莫非你是安琪拉？要&lt;strong style=&quot;color:red;&quot;&gt;“缝缝补补又是一年？”&lt;/strong&gt;）&lt;/li&gt;
      &lt;li&gt;那么，你的思维是什么？是为了彰显你的研发能力以及代码量而生的吗？
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;正解：敢于推倒架构设计，架构师不是神人，也会有架构漏洞以及不足点，要以发展眼光看待问题，而不是尊崇。（毕竟活在前人阴影下很累。）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;4人云亦云普度众生&quot;&gt;4.人云亦云普度众生&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong style=&quot;color:blue;&quot;&gt;人云亦云，技术选型从来都是拍脑袋 or 看了几篇博客？？？&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;不经大脑思考或者浅尝辄止就自认为了如指掌某门技术？是骡子是马敢拉出来溜溜吗？走两步？&lt;/li&gt;
      &lt;li&gt;人云亦云？如果没有自己想法就坦言，慢慢培养自己的意识，而不是随声附和。（&lt;strong style=&quot;color:red;&quot;&gt;要有对外我是“辅助位”，对内深藏不露。&lt;/strong&gt;）&lt;/li&gt;
      &lt;li&gt;张口就来？这个用Flink？这个用Clickhouse？这个用RabbitMQ？你调研了吗？有数据依据吗？请发出来你的测试报告以及数据凭证。
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;正解：往往最站不住脚的是空口无凭。聪明的人，会直接发出上帝之手（有数据+测试报告），让你无力反驳。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;5如法炮制乐此不疲&quot;&gt;5.如法炮制乐此不疲&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong style=&quot;color:blue;&quot;&gt;经常会遇到一类人，总喜欢如法炮制，一个方案解万千难题。&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;你可否考虑过在同一套架构模式下，前后二者数据量级 or 数据埋点相关的差距？&lt;/li&gt;
      &lt;li&gt;你可否考虑过在同一套代码逻辑下，前后二者producer-consumer部署方案不同会有所差别？&lt;/li&gt;
      &lt;li&gt;你可否考虑过在用一个HQL ETL问题时，前后时间跨度已经截然不同（一年前和一年后数据量差距有多大？dt范围有多大？）？
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;正解：世间没有银弹。我们总是习惯于如法炮制，一套方法冠以多用，但是往往得不偿失。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
说了这么多，其实只是想表达一个点：多思考，多复盘，多反思，跳出极限。&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Dec 2020 00:00:00 +0800</pubDate>
        <link>http://www.buildupchao.cn/tech-talking/2020/12/08/what-do-we-lack-of-ability.html</link>
        <guid isPermaLink="true">http://www.buildupchao.cn/tech-talking/2020/12/08/what-do-we-lack-of-ability.html</guid>
        
        <category>tech-talking</category>
        
        
        <category>tech-talking</category>
        
      </item>
    
      <item>
        <title>Log4j2日志滚动策略TimeBasedTriggeringPolicy的魔鬼槽点</title>
        <description>&lt;p&gt;&lt;code&gt;TimeBasedTriggeringPolicy&lt;/code&gt;参数说明：&lt;/p&gt;

&lt;!-- more --&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;参数名称&lt;/th&gt;
      &lt;th&gt;类型&lt;/th&gt;
      &lt;th&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;interval&lt;/td&gt;
      &lt;td&gt;integer&lt;/td&gt;
      &lt;td&gt;根据日期格式中最具体的时间单位来决定应该多久发生一次rollover。例如，在日期模式中小时为具体的时间单位，那么每4小时会发生4次rollover，默认值为1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;modulate&lt;/td&gt;
      &lt;td&gt;boolean&lt;/td&gt;
      &lt;td&gt;表示是否调整时间间隔以使在时间间隔边界发生下一个rollover。例如：假设小时为具体的时间单元，当前时间为上午3点，时间间隔为4，第一次发送rollover是在上午4点，接下来是上午8点，接着是中午，接着是下午4点等发生。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;由上图我们展开两个测试用例：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;case 1：如何实现天粒度切分日志？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/Java/log4j2/day_split.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;case 2: 如何实现小时粒度（基于秒）切分日志？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/Java/log4j2/seconds_split.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以，log4j2中&lt;code&gt;TimeBasedTriggeringPolicy&lt;/code&gt;切分文件策略，是基于filePattern中的&lt;code&gt;%d{yyyy-MM-dd-HH-mm-ss}&lt;/code&gt;来决定到底采用哪种时间单位（天、小时、分钟、秒等）。&lt;/p&gt;

&lt;p&gt;那上面两种方式有没有错误呢？？？逻辑上没问题，但是case 2却没有达到我们想要的，我们其实是想要达到凌晨0点切分时间的（而不是推延24小时）。&lt;/p&gt;

&lt;p&gt;我们已经指定了 &lt;code&gt;modulus&lt;/code&gt; （modulus就是modulate）为true，应该以0点自动校准进行文件切分时间规划的，然而，当我们设置了86400秒（也就是24小时）一切分的时候，却没有达到0点切分的目的，而是项目启动的当前时间推算24小时，这是为什么呢？&lt;/p&gt;

&lt;p&gt;我们再来看下Log4j2基于时间切分逻辑底层&lt;code&gt;org.apache.logging.log4j.core.appender.rolling.PatternProcessor&lt;/code&gt;判断逻辑(部分截图)：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/Java/log4j2/PatternProcessor-1.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/Java/log4j2/PatternProcessor-2.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们再进入&lt;code&gt;increment&lt;/code&gt;函数查看下实现：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/Java/log4j2/PatternProcessor-3.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;哦~原来如此，当我们指定了&lt;code&gt;modulus&lt;/code&gt;时，它是根据我们的filePattern最后一位为基准0进行推延计算的。&lt;/p&gt;

&lt;p&gt;也就是说，我们可以得出来如下的表格：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;filePattern&lt;/th&gt;
      &lt;th&gt;increment&lt;/th&gt;
      &lt;th&gt;prevFileTime（采用非时间戳方式表示）&lt;/th&gt;
      &lt;th&gt;nextFileTime（采用非时间戳方式表示）&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;yyyy-MM-dd-HH-mm-ss&lt;/td&gt;
      &lt;td&gt;7200&lt;/td&gt;
      &lt;td&gt;2020-12-10-12-56-35&lt;/td&gt;
      &lt;td&gt;2020-12-10-14-56-00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;yyyy-MM-dd-HH-mm&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;2020-12-10-12-56-35&lt;/td&gt;
      &lt;td&gt;2020-12-10-14-00-00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;yyyy-MM-dd-HH&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2020-12-10-12-56-35&lt;/td&gt;
      &lt;td&gt;2020-12-10-14-00-00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;yyyy-MM-dd&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2020-12-10-12-56-35&lt;/td&gt;
      &lt;td&gt;2020-12-11-00-00-00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;yyyy-MM&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2020-12-10-12-56-35&lt;/td&gt;
      &lt;td&gt;2021-01-01-00-00-00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;yyyy&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2020-12-10-12-56-35&lt;/td&gt;
      &lt;td&gt;2021-01-01-00-00-00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;所以，这种略有些逆人性的实现逻辑，真的很容易踩坑。&lt;/p&gt;

&lt;p&gt;但是说到底还是对log4j2切分策略不够熟悉，导致使用上有所偏差。&lt;/p&gt;

&lt;p&gt;等后面有空，再对log4j2的disruptor使用也进行详细总结~&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Dec 2020 00:00:00 +0800</pubDate>
        <link>http://www.buildupchao.cn/java/2020/12/08/how-log4j2-split-log-cause.html</link>
        <guid isPermaLink="true">http://www.buildupchao.cn/java/2020/12/08/how-log4j2-split-log-cause.html</guid>
        
        <category>java</category>
        
        <category>log4j2</category>
        
        
        <category>java</category>
        
      </item>
    
      <item>
        <title>Log4j2是如何切分日志的呢？</title>
        <description>&lt;p&gt;简化语言，一切用图来表达吧：&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/Java/log4j2/Flume-Log4j2-split-log-logic-open.png?raw=true&quot; alt=&quot;log4j2日志切分流程图&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Dec 2020 00:00:00 +0800</pubDate>
        <link>http://www.buildupchao.cn/java/2020/12/07/how-log4j2-split-log.html</link>
        <guid isPermaLink="true">http://www.buildupchao.cn/java/2020/12/07/how-log4j2-split-log.html</guid>
        
        <category>java</category>
        
        <category>log4j2</category>
        
        
        <category>java</category>
        
      </item>
    
      <item>
        <title>数仓采集之Flume改造二三事（三）</title>
        <description>
</description>
        <pubDate>Mon, 09 Nov 2020 00:00:00 +0800</pubDate>
        <link>http://www.buildupchao.cn/flume/2020/11/09/flume-performance-optimize-3.html</link>
        <guid isPermaLink="true">http://www.buildupchao.cn/flume/2020/11/09/flume-performance-optimize-3.html</guid>
        
        <category>flume</category>
        
        <category>bigdata</category>
        
        <category>datawarehouse</category>
        
        
        <category>flume</category>
        
      </item>
    
      <item>
        <title>数仓采集之Flume改造二三事（二）</title>
        <description>
</description>
        <pubDate>Sun, 08 Nov 2020 00:00:00 +0800</pubDate>
        <link>http://www.buildupchao.cn/flume/2020/11/08/flume-performance-optimize-2.html</link>
        <guid isPermaLink="true">http://www.buildupchao.cn/flume/2020/11/08/flume-performance-optimize-2.html</guid>
        
        <category>flume</category>
        
        <category>bigdata</category>
        
        <category>datawarehouse</category>
        
        
        <category>flume</category>
        
      </item>
    
      <item>
        <title>数仓采集之Flume改造二三事（一）</title>
        <description>
</description>
        <pubDate>Sat, 07 Nov 2020 00:00:00 +0800</pubDate>
        <link>http://www.buildupchao.cn/flume/2020/11/07/flume-performance-optimize-1.html</link>
        <guid isPermaLink="true">http://www.buildupchao.cn/flume/2020/11/07/flume-performance-optimize-1.html</guid>
        
        <category>flume</category>
        
        <category>bigdata</category>
        
        <category>datawarehouse</category>
        
        
        <category>flume</category>
        
      </item>
    
      <item>
        <title>没有Spring cron时该怎么定点执行定时任务？</title>
        <description>&lt;p&gt;最近有个小需求，在普通Java项目里面，不能借助于Spring，也不能使用复杂的jar，来实现cron定点定时任务。&lt;br /&gt;
通过查询资料，发现一个好用的工具：hutool&lt;br /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-maven&quot; data-lang=&quot;maven&quot;&gt;&amp;lt;!-- https://mvnrepository.com/artifact/cn.hutool/hutool-cron --&amp;gt;
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;cn.hutool&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;hutool-cron&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;5.2.2&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;参考资料如下：&lt;a href=&quot;https://www.bookstack.cn/read/hutool/0f082d6e35363da6.md&quot; target=&quot;_blank&quot;&gt;https://www.bookstack.cn/read/hutool/0f082d6e35363da6.md&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;普通应用里面，只需引入这个简单的jar包，仅仅36KB，就可以方便快捷的实现cron方式执行任务。&lt;/p&gt;
</description>
        <pubDate>Tue, 17 Mar 2020 00:00:00 +0800</pubDate>
        <link>http://www.buildupchao.cn/java/2020/03/17/normal-java-cron-task.html</link>
        <guid isPermaLink="true">http://www.buildupchao.cn/java/2020/03/17/normal-java-cron-task.html</guid>
        
        <category>java</category>
        
        <category>cron</category>
        
        
        <category>java</category>
        
      </item>
    
      <item>
        <title>为何不推荐直接采用 Executors.new 线程池的方式？</title>
        <description>&lt;p&gt;众所周知，Java并发编程是一大难点所在。&lt;br /&gt;
其实并不是我们不懂并发原理，而是我们往往忽略了细节，然而，&lt;strong style=&quot;color:red&quot;&gt;“魔鬼总在细节中”！&lt;/strong&gt;;&lt;br /&gt;
&lt;br /&gt;
作为以Java作为主语言的研发，应该都知道 Executors.new 各种线程池的时候（ScheduledThreadPool除外），底层都是通过 ThreadPoolExecutor来实现的。&lt;/p&gt;

&lt;p&gt;&lt;strong style=&quot;color:green;&quot;&gt;[延拓]&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;code&gt;ThreadPoolExecutor&lt;/code&gt;的5项基本参数为：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;int corePoolSize：核心线程数&lt;/li&gt;
  &lt;li&gt;int maximumPoolSize：线程池最大可拥有的线程数（最高并发量）&lt;/li&gt;
  &lt;li&gt;long keepAliveTime：线程idle态的最大存留时间&lt;/li&gt;
  &lt;li&gt;TimeUnit unit：时间单位，配合 keepAliveTime 使用&lt;/li&gt;
  &lt;li&gt;BlockingQueue&lt;Runnable&gt; workQueue：任务队列（当任务量打满核心线程数 or 最大线程数 时，用来存放排队等待执行的任务）&lt;/Runnable&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;
以下主要以 &lt;code&gt;Executors.newFixedThreadPool(nThreads)&lt;/code&gt;进行分析：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/Java/concurrent/threadpool/newFixedThreadPool.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由上图，我们可以明显得出两项信息：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;线程池中线程不会进行回收，会一直存活，即使都一直处于idle状态（因为corePoolSize和maximumPoolSize被设置为等大）&lt;/li&gt;
  &lt;li&gt;任务队列直接采用&lt;code&gt;new LinkedBlockingQueue&amp;lt;Runnable&amp;gt;()&lt;/code&gt;方式，是无界的，也就是说任务量飙升时会存在内存溢出风险&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;那还有没有其他信息呢？？？当然！！！
我们再来看通过&lt;code&gt;Executors.newFixedThreadPool&lt;/code&gt;方式调用&lt;code&gt;ThreadPoolExecutor&lt;/code&gt;时，存在哪些隐性操作：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/Java/concurrent/threadpool/Executors.newFixedThreadPool.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/Java/concurrent/threadpool/Executors.defaultThreadFactory.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;啊！！！你看，内部直接采用了&lt;code&gt;Executors.defaultThreadFactory()&lt;/code&gt;，它会做些什么？是以什么样的方式来给我们提供工作线程呢？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/Java/concurrent/threadpool/DefaultThreadFactory.newThread.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;发现问题严重性了吗？默认提供的工作线程均为&lt;strong style=&quot;color:red;&quot;&gt;非守护线程&lt;/strong&gt;！&lt;br /&gt;&lt;br /&gt;
也就是说，如果一旦发生意外，主线程都已经退出了，如果此时我们因为异常而没有关闭线程池的话，进程会作为一个僵尸进程一直存在的！！！&lt;/p&gt;

&lt;p&gt;这也是近期排查线上问题时，发现有些小伙伴写代码太过于随意，根本不考虑边界情况以及异常case。
&lt;br /&gt;&lt;br /&gt;
所以，我们平时设计并发编程，采用线程或线程池的时候，多一些思考，可能就会有不一样的结果。
&lt;br /&gt;&lt;br /&gt;
在这里我也只是抛砖引玉，有兴趣的话，可以自己尝试去分析下&lt;code&gt;Executors.new&lt;/code&gt;的另外几种方式又会存在哪些潜在风险呢？&lt;/p&gt;
</description>
        <pubDate>Mon, 17 Feb 2020 00:00:00 +0800</pubDate>
        <link>http://www.buildupchao.cn/java/2020/02/17/why-donot-use-Executors.new.html</link>
        <guid isPermaLink="true">http://www.buildupchao.cn/java/2020/02/17/why-donot-use-Executors.new.html</guid>
        
        <category>java</category>
        
        <category>threadpool</category>
        
        <category>Executors</category>
        
        
        <category>java</category>
        
      </item>
    
      <item>
        <title>Flink的Watermark是如何治理数据乱序的？</title>
        <description>
</description>
        <pubDate>Thu, 16 Jan 2020 00:00:00 +0800</pubDate>
        <link>http://www.buildupchao.cn/flink/2020/01/16/deep-in-cep-and-watermark-for-flink.html</link>
        <guid isPermaLink="true">http://www.buildupchao.cn/flink/2020/01/16/deep-in-cep-and-watermark-for-flink.html</guid>
        
        <category>Flink</category>
        
        <category>大数据</category>
        
        
        <category>flink</category>
        
      </item>
    
      <item>
        <title>Flink Table API &amp; SQL</title>
        <description>&lt;h2 id=&quot;table-api--sql介绍&quot;&gt;Table API &amp;amp; SQL介绍&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Apache Flink具有两个关系API：表API和SQL，用于统一流和批处理。Table API是Scala和Java的语言集成查询API，查询允许组合关系运算符，例如过滤和连接。Flink SQL支持标准的SQL语法。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Table API和SQL接口彼此集成，FLink的DataStream和DataSet API亦是如此。你可以轻松地基于API构建的所有API和库之间切换。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;注意，到目前最新版本位置，Table API和SQL还有很多功能正在开发中。并非[Table API, SQL]和[Stream, Batch]输入的每种组合都支持所有操作。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- more --&gt;

&lt;h3 id=&quot;1为什么需要table-api--sql&quot;&gt;1.为什么需要Table API &amp;amp; SQL&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Table API是一种关系型API，类SQL的API，用户可以像操作表一样地操作数据，非常的直观和方便。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SQL作为一个”人所皆知”的语言，如果一个引擎提供SQL，它将很容易被人们接受。这已经是业界很常见的现象。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Table &amp;amp; SQL API还有另外一个职责，就是流处理和批处理统一的API层。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2table-api--sql流处理概述&quot;&gt;2.Table API &amp;amp; SQL流处理概述&lt;/h3&gt;

&lt;p&gt;Flink的Table API和SQL支持是用于批处理和流处理的统一API。这意味着Table API和SQL查询具有相同的语义，无论它们的输入是有界批量输入还是无界流输入。因为关系代数（relational algebra）和SQL最初是为批处理而设计的，所以对于无界流输入的关系查询不像有界批输入上的关系查询那样容易理解。&lt;/p&gt;

&lt;h4 id=&quot;21流数据上的关系查询&quot;&gt;2.1.流数据上的关系查询&lt;/h4&gt;

&lt;p&gt;SQL和Relational algebra并没有考虑到流数据。因此，在关系代数（和SQL）和流处理之间有一些概念上的差距。
&lt;br /&gt;&lt;/p&gt;
&lt;table&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;关系代数/SQL&lt;/th&gt;
            &lt;th&gt;流处理&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;关系（或表）是有界的（多）元组的集合&lt;/td&gt;
            &lt;td&gt;流式无界的元组序列&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;对批处理数据执行的查询（例如，关系数据库中的表）可以访问完整的输入数据&lt;/td&gt;
            &lt;td&gt;流式查询在启动时无法访问所有数据，必须等待流式传输数据&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;批处理查询在生成固定大小的结果后终止&lt;/td&gt;
            &lt;td&gt;流式查询会根据收到的记录不断更新其结果，并且永远不会完成&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;22动态表和连续查询&quot;&gt;2.2动态表和连续查询&lt;/h4&gt;

&lt;p&gt;动态表（Dynamic table）是Flink Table API和SQL支持流数据的核心概念。与表示批处理数据的静态表（static table）相比，动态表会随时间而变化，并且可以像静态批处理表一样查询。查询动态表会生成连续查询（Continuous Query）。连续查询永远不会终止并生成动态表作为结果。查询不断更新其结果以反映其输入表的更改。
&lt;br /&gt;
&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/blog/flink/flink-foundation/dynamic-table-and-continuous-query-1.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;23在流上定义表&quot;&gt;2.3在流上定义表&lt;/h4&gt;

&lt;p&gt;为了使用关系查询处理流，必须将其转换为表。从概念上将，流的每个记录都被解释为对结果表的INSERT修改。下图显示了点击事件流（左侧）如何转换为表（右侧）。随着更多的点击事件的插入，结果表不断增长。
&lt;br /&gt;
&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/blog/flink/flink-foundation/dynamic-table-and-continuous-query-2.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;24连续查询&quot;&gt;2.4连续查询&lt;/h4&gt;

&lt;p&gt;在动态表上进行连续查询，并生成新的动态表。与批查询相反，连续查询不会停止更新其结果表。在任何时间点，连续查询的结果在语义上等同于在输入表的快照上以批处理模式执行的相同查询的结果。
&lt;br /&gt;
&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/blog/flink/flink-foundation/dynamic-table-and-continuous-query-3.png?raw=true&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/blog/flink/flink-foundation/dynamic-table-and-continuous-query-4.png?raw=true&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;25表转换到流&quot;&gt;2.5表转换到流&lt;/h4&gt;

&lt;p&gt;流查询的结果表将被动态更新，即，随着新纪录到达查询的输入流，它也发生变化。因此，将这样的动态查询转换成的DataStream需要对表的更新进行编码。
&lt;br /&gt;
将表转换为数据流有两种方式：
&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Append-only Mode：只有在动态Table仅通过INSERT更新修改时才能使用此模式，即它仅附加，并且以前发出的结果永远不会更新。如果更新或删除操作使用追加模式会失败报错。&lt;/li&gt;
  &lt;li&gt;Retract Mode：始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入或撤回，返回true代表数据插入，false代表数据的撤回。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/blog/flink/flink-foundation/dynamic-table-and-continuous-query-5.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3flink-sql-connector&quot;&gt;3.Flink SQL Connector&lt;/h3&gt;

&lt;p&gt;Flink的表API和SQL程序可以连接到其他外部系统来读写批处理表和流表。&lt;br /&gt;
Table source提供对存储在外部系统（如数据库、键值存储、消息队列或文件系统）中的数据的访问。&lt;br /&gt;
Table Sink将表发送到外部存储系统。
&lt;br /&gt;
&lt;img src=&quot;https://github.com/buildupchao/ImgStore/blob/master/blog/flink/flink-foundation/dynamic-table-and-continuous-query-6.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;4flink-table-api&quot;&gt;4.Flink Table API&lt;/h3&gt;

&lt;p&gt;Table API使用一个Scala和Java的语言集成查询API，是基于Table类。Table类代表了一个流或者批表，并提供方法来使用关系型操作。这些方法返回一个新的Table对象，这个新的Table对象代表着输入的Table应用关系型操作后的结果。
&lt;br /&gt;
&lt;a href=&quot;https://github.com/buildupchao/flink-examples/blob/master/src/main/java/com/buildupchao/flinkexamples/batch/api/BatchTableExample.java&quot;&gt;代码：批表案例&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/buildupchao/flink-examples/blob/master/src/main/java/com/buildupchao/flinkexamples/stream/StreamTableApiAndSqlExample.java&quot;&gt;代码：流表案例&lt;/a&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;5flink-sql&quot;&gt;5.Flink SQL&lt;/h3&gt;

&lt;p&gt;Flink SQL集成是基于Apache Calcite，Apache Calcite实现了标准的SQL。
&lt;br /&gt;
&lt;a href=&quot;https://github.com/buildupchao/flink-examples/blob/master/src/main/java/com/buildupchao/flinkexamples/batch/sql/BatchOrderCaseSQLExample.java&quot;&gt;代码：订单案例&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;资料快链&quot;&gt;资料快链&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/&quot;&gt;官网：Table API &amp;amp; SQL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/sql.html&quot;&gt;官网：Flink保留字列表&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 13 Jan 2020 00:00:00 +0800</pubDate>
        <link>http://www.buildupchao.cn/flink/2020/01/13/flink-table-api-and-sql.html</link>
        <guid isPermaLink="true">http://www.buildupchao.cn/flink/2020/01/13/flink-table-api-and-sql.html</guid>
        
        <category>Flink</category>
        
        <category>大数据</category>
        
        
        <category>flink</category>
        
      </item>
    
  </channel>
</rss>
